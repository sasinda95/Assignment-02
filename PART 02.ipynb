{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) Monte Carlo method with exploring starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy with Exploring Starts:\n",
      "→ ↑ ← → ↑\n",
      "→ ↑ ↑ ← ↑\n",
      "→ ↑ ← ↑ ↑\n",
      "→ → ↑ ↑ ←\n",
      "↑ ↑ ↑ ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "MAX_STEPS = 100  # Maximum number of steps per episode\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.full((grid_size, grid_size), -0.2)  # Initialize rewards with -0.2 for each move\n",
    "rewards[0, 1] = 5   # Blue square\n",
    "rewards[0, 4] = 2.5  # Green square\n",
    "\n",
    "# Define transitions for special squares\n",
    "transitions = {}\n",
    "transitions[(0, 1)] = [(5, (4, 2))]   # Blue -> Red\n",
    "transitions[(0, 4)] = [(2.5, (4, 2)), (2.5, (4, 4))]  # Green -> Red or Yellow\n",
    "\n",
    "# Define terminal states\n",
    "terminal_states = [(4, 0), (2, 4)]  # Black squares are terminal states\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, down, left, right\n",
    "\n",
    "np.random.seed(42)  # Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Function to get the next state and reward\n",
    "def get_next_state_reward(state, action):\n",
    "    if state in terminal_states:\n",
    "        return (0, state)  # No reward, remain in terminal state\n",
    "    if state in transitions:\n",
    "        transition = transitions[state]\n",
    "        idx = np.random.choice(len(transition))  # Choose one of the possible transitions\n",
    "        return transition[idx]\n",
    "    else:\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            return (-0.5, state)  # Stepping off the grid\n",
    "        return (-0.2, next_state)  # Normal move\n",
    "\n",
    "# Initialize policy and value function\n",
    "π = np.ones((grid_size, grid_size, len(actions))) / len(actions)  # Equiprobable policy\n",
    "Q = np.zeros((grid_size, grid_size, len(actions)))  # Initialize action-value function\n",
    "Returns = [[[] for _ in range(len(actions))] for _ in range(grid_size * grid_size)]  # Store returns for each state-action pair\n",
    "\n",
    "# Function to generate an episode using exploring starts\n",
    "def generate_episode(π):\n",
    "    S0 = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "    A0 = random.choice(actions)\n",
    "    episode = [(S0, A0, 0)]  # (state, action, reward)\n",
    "    \n",
    "    state = S0\n",
    "    action = A0\n",
    "    steps = 0  # Add a step counter to prevent infinite loops\n",
    "    while state not in terminal_states and steps < MAX_STEPS:  # Limit the number of steps to prevent infinite loops\n",
    "        reward, next_state = get_next_state_reward(state, action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if state in terminal_states:\n",
    "            break\n",
    "        action = random.choices(actions, π[state[0], state[1]])[0]\n",
    "        steps += 1  # Increment the step counter\n",
    "    \n",
    "    return episode\n",
    "\n",
    "# Monte Carlo method with exploring starts\n",
    "def monte_carlo_es(π, Q, Returns, num_episodes):\n",
    "    avg_returns_per_episode = []\n",
    "    for episode_num in range(num_episodes):\n",
    "        episode = generate_episode(π)\n",
    "        G = 0  # Initialize return\n",
    "        visited = set()  # Track visited state-action pairs\n",
    "        \n",
    "        for t in reversed(range(len(episode) - 1)):\n",
    "            St, At, Rt = episode[t]\n",
    "            G = gamma * G + episode[t + 1][2]  # episode[t + 1][2] is the reward\n",
    "            state_action = (St, actions.index(At))\n",
    "            if state_action not in visited:\n",
    "                state_index = St[0] * grid_size + St[1]\n",
    "                Returns[state_index][actions.index(At)].append(G)\n",
    "                Q[St[0], St[1], actions.index(At)] = np.mean(Returns[state_index][actions.index(At)])\n",
    "                visited.add(state_action)\n",
    "        \n",
    "        # Calculate and store the average return of the current episode\n",
    "        avg_returns_per_episode.append(G)\n",
    "        \n",
    "        # Update policy\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                best_action = np.argmax(Q[i, j])\n",
    "                π[i, j] = np.eye(len(actions))[best_action]\n",
    "    \n",
    "    return π, Q\n",
    "\n",
    "# Perform Monte Carlo with exploring starts\n",
    "optimal_policy_es, optimal_Q_es = monte_carlo_es(π, Q, Returns, num_episodes=20000)\n",
    "\n",
    "# Print the optimal policy using text\n",
    "action_symbols = {(-1, 0): '↑', (1, 0): '↓', (0, -1): '←', (0, 1): '→'}\n",
    "print(\"Optimal Policy with Exploring Starts:\")\n",
    "for i in range(grid_size):\n",
    "    row = []\n",
    "    for j in range(grid_size):\n",
    "        best_action_idx = np.argmax(optimal_policy_es[i, j])\n",
    "        best_action = actions[best_action_idx]\n",
    "        row.append(action_symbols[best_action])\n",
    "    print(\" \".join(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2). Monte Carlo method without Exploring Starts but using ϵ-soft approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy with Epsilon-Soft Policy:\n",
      "↑ ↓ ← ← ↓\n",
      "→ ↑ ↑ ↑ ←\n",
      "↑ ↑ ↑ ↑ ↑\n",
      "↑ ← ↑ ← ←\n",
      "↑ ↓ ↑ ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.full((grid_size, grid_size), -0.2)\n",
    "rewards[0, 1] = 5   # Blue square\n",
    "rewards[0, 4] = 2.5  # Green square\n",
    "\n",
    "# Define transitions for special squares\n",
    "transitions = {}\n",
    "transitions[(0, 1)] = [(5, (4, 2))]   # Blue -> Red\n",
    "transitions[(0, 4)] = [(2.5, (4, 2)), (2.5, (4, 4))]  # Green -> Red or Yellow\n",
    "\n",
    "# Define terminal states\n",
    "terminal_states = [(4, 0), (2, 4)]\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "action_indices = {(-1, 0): 0, (1, 0): 1, (0, -1): 2, (0, 1): 3}\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Function to get the next state and reward\n",
    "def get_next_state_reward(state, action):\n",
    "    if state in terminal_states:\n",
    "        return (0, state)\n",
    "    if state in transitions:\n",
    "        transition = transitions[state]\n",
    "        idx = np.random.choice(len(transition))\n",
    "        return transition[idx]\n",
    "    else:\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            return (-0.5, state)  # Stepping off the grid\n",
    "        return (-0.2, next_state)  # Normal move\n",
    "\n",
    "# Initialize policy and value function\n",
    "π = np.ones((grid_size, grid_size, len(actions))) / len(actions)  # ϵ-soft policy\n",
    "Q = np.zeros((grid_size, grid_size, len(actions)))  # Initialize action-value function\n",
    "Returns = [[[] for _ in range(len(actions))] for _ in range(grid_size * grid_size)]  # Store returns for each state-action pair\n",
    "\n",
    "# Function to generate an episode following the epsilon-soft policy\n",
    "def generate_episode(π):\n",
    "    state = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "    episode = []\n",
    "    \n",
    "    steps = 0  # Add a step counter to prevent infinite loops\n",
    "    while state not in terminal_states and steps < 100:  # Limit the number of steps to prevent infinite loops\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)  # Random action with probability epsilon\n",
    "        else:\n",
    "            action = random.choices(actions, π[state[0], state[1]])[0]  # Greedy action with probability 1-epsilon\n",
    "        reward, next_state = get_next_state_reward(state, action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        steps += 1  # Increment the step counter\n",
    "    \n",
    "    return episode\n",
    "\n",
    "# Monte Carlo method with epsilon-soft policy\n",
    "def monte_carlo_eps_soft(π, Q, Returns, num_episodes):\n",
    "    for episode_num in range(num_episodes):\n",
    "        episode = generate_episode(π)\n",
    "        G = 0  # Initialize return\n",
    "        visited = set()  # Track visited state-action pairs\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            state_action = (state, actions.index(action))\n",
    "            if state_action not in visited:\n",
    "                state_index = state[0] * grid_size + state[1]\n",
    "                Returns[state_index][actions.index(action)].append(G)\n",
    "                Q[state[0], state[1], actions.index(action)] = np.mean(Returns[state_index][actions.index(action)])\n",
    "                visited.add(state_action)\n",
    "                \n",
    "                best_action = np.argmax(Q[state[0], state[1]])\n",
    "                for a in range(len(actions)):\n",
    "                    if a == best_action:\n",
    "                        π[state[0], state[1], a] = 1 - epsilon + epsilon / len(actions)\n",
    "                    else:\n",
    "                        π[state[0], state[1], a] = epsilon / len(actions)\n",
    "    \n",
    "    return π, Q\n",
    "\n",
    "# Initialize policy and value function for epsilon-soft\n",
    "policy_eps_soft = np.ones((grid_size, grid_size, len(actions))) / len(actions)\n",
    "Q_eps_soft = np.zeros((grid_size, grid_size, len(actions)))\n",
    "returns_eps_soft = [[[] for _ in range(len(actions))] for _ in range(grid_size * grid_size)]\n",
    "\n",
    "# Perform Monte Carlo with epsilon-soft policy\n",
    "optimal_policy_eps_soft, optimal_Q_eps_soft = monte_carlo_eps_soft(policy_eps_soft, Q_eps_soft, returns_eps_soft, num_episodes=20000)\n",
    "\n",
    "# Print the optimal policy using text\n",
    "action_symbols = {(-1, 0): '↑', (1, 0): '↓', (0, -1): '←', (0, 1): '→'}\n",
    "print(\"Optimal Policy with Epsilon-Soft Policy:\")\n",
    "for i in range(grid_size):\n",
    "    row = []\n",
    "    for j in range(grid_size):\n",
    "        best_action_idx = np.argmax(optimal_policy_eps_soft[i, j])\n",
    "        best_action = actions[best_action_idx]\n",
    "        row.append(action_symbols[best_action])\n",
    "    print(\" \".join(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1). Behaviour policy with equiprobable moves to learn an optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy with Importance Sampling:\n",
      "→ ↑ ← → ←\n",
      "↑ ↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ → ↑\n",
      "↓ ← ↑ ↑ ↑\n",
      "↑ ← ← ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Epsilon for epsilon-greedy policy\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.full((grid_size, grid_size), -0.2)\n",
    "rewards[0, 1] = 5   # Blue square\n",
    "rewards[0, 4] = 2.5  # Green square\n",
    "\n",
    "# Define transitions for special squares\n",
    "transitions = {}\n",
    "transitions[(0, 1)] = [(5, (4, 2))]   # Blue -> Red\n",
    "transitions[(0, 4)] = [(2.5, (4, 2)), (2.5, (4, 4))]  # Green -> Red or Yellow\n",
    "\n",
    "# Define terminal states\n",
    "terminal_states = [(4, 0), (2, 4)]\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "action_indices = {(-1, 0): 0, (1, 0): 1, (0, -1): 2, (0, 1): 3}\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Function to get the next state and reward\n",
    "def get_next_state_reward(state, action):\n",
    "    if state in terminal_states:\n",
    "        return (0, state)\n",
    "    if state in transitions:\n",
    "        transition = transitions[state]\n",
    "        idx = np.random.choice(len(transition))\n",
    "        return transition[idx]\n",
    "    else:\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            return (-0.5, state)  # Stepping off the grid\n",
    "        return (-0.2, next_state)  # Normal move\n",
    "\n",
    "# Initialize the behavior policy and value function\n",
    "behavior_policy = np.ones((grid_size, grid_size, len(actions))) / len(actions)  # Equiprobable policy\n",
    "target_policy = np.ones((grid_size, grid_size, len(actions))) / len(actions)  # Initial target policy\n",
    "Q = np.zeros((grid_size, grid_size, len(actions)))  # Initialize action-value function\n",
    "Returns = [[[] for _ in range(len(actions))] for _ in range(grid_size * grid_size)]  # Store returns for each state-action pair\n",
    "\n",
    "# Function to generate an episode following the behavior policy\n",
    "def generate_episode(policy):\n",
    "    state = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "    episode = []\n",
    "    \n",
    "    steps = 0  # Add a step counter to prevent infinite loops\n",
    "    while state not in terminal_states and steps < 100:  # Limit the number of steps to prevent infinite loops\n",
    "        action = random.choices(actions, policy[state[0], state[1]])[0]  # Choose action based on the policy\n",
    "        reward, next_state = get_next_state_reward(state, action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        steps += 1  # Increment the step counter\n",
    "    \n",
    "    return episode\n",
    "\n",
    "# Monte Carlo method with importance sampling\n",
    "def monte_carlo_importance_sampling(behavior_policy, target_policy, Q, Returns, num_episodes):\n",
    "    for episode_num in range(num_episodes):\n",
    "        episode = generate_episode(behavior_policy)\n",
    "        G = 0  # Initialize return\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            state_action = (state, actions.index(action))\n",
    "            state_index = state[0] * grid_size + state[1]\n",
    "            \n",
    "            if state_action not in [(e[0], actions.index(e[1])) for e in episode[:t]]:\n",
    "                Returns[state_index][actions.index(action)].append(G)\n",
    "                Q[state[0], state[1], actions.index(action)] = np.mean(Returns[state_index][actions.index(action)])\n",
    "                \n",
    "                best_action = np.argmax(Q[state[0], state[1]])\n",
    "                for a in range(len(actions)):\n",
    "                    if a == best_action:\n",
    "                        target_policy[state[0], state[1], a] = 1 - epsilon + epsilon / len(actions)\n",
    "                    else:\n",
    "                        target_policy[state[0], state[1], a] = epsilon / len(actions)\n",
    "    \n",
    "    return target_policy, Q\n",
    "\n",
    "# Initialize policy and value function for importance sampling\n",
    "policy_importance_sampling = np.ones((grid_size, grid_size, len(actions))) / len(actions)\n",
    "Q_importance_sampling = np.zeros((grid_size, grid_size, len(actions)))\n",
    "returns_importance_sampling = [[[] for _ in range(len(actions))] for _ in range(grid_size * grid_size)]\n",
    "\n",
    "# Perform Monte Carlo with importance sampling\n",
    "optimal_policy_importance_sampling, optimal_Q_importance_sampling = monte_carlo_importance_sampling(behavior_policy, policy_importance_sampling, Q_importance_sampling, returns_importance_sampling, num_episodes=20000)\n",
    "\n",
    "# Print the optimal policy using text\n",
    "action_symbols = {(-1, 0): '↑', (1, 0): '↓', (0, -1): '←', (0, 1): '→'}\n",
    "print(\"Optimal Policy with Importance Sampling:\")\n",
    "for i in range(grid_size):\n",
    "    row = []\n",
    "    for j in range(grid_size):\n",
    "        best_action_idx = np.argmax(optimal_policy_importance_sampling[i, j])\n",
    "        best_action = actions[best_action_idx]\n",
    "        row.append(action_symbols[best_action])\n",
    "    print(\" \".join(row))\n",
    "\n",
    "# Print the Q-values for each state-action pair\n",
    "#print(\"\\nQ-values:\")\n",
    "#for i in range(grid_size):\n",
    "    #for j in range(grid_size):\n",
    "        #print(f\"State ({i},{j}): {Q_importance_sampling[i, j]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1). Permute the locations of the green and blue squares with probability 0.1, while preserving the rewards and transition structure as before. Use policy iteration to determine a suitable policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration with Permutation:\n",
      "\n",
      "Policy Iteration without Permutation:\n",
      "\n",
      "Optimal Policy with Permutation (using arrows):\n",
      "→ → ← ← ←\n",
      "↑ ↑ ↑ ← ↑\n",
      "↑ ↑ ↑ ← ↑\n",
      "↑ ↑ ↑ ← ←\n",
      "↑ ↑ ↑ ← ↑\n",
      "\n",
      "Optimal Value Function with Permutation:\n",
      "[[45.39788919 42.92799473 45.78159499 24.01287014 22.61222663]\n",
      " [42.92799473 43.28159499 43.29251524 22.61222663 23.9816153 ]\n",
      " [40.58159499 40.91751524 40.92788948 21.2816153   0.        ]\n",
      " [38.35251524 38.67163948 38.681495   20.01753453 18.81665781]\n",
      " [ 0.         36.5380575  36.54742025 18.81665781 17.67582492]]\n",
      "\n",
      "Optimal Policy without Permutation (using arrows):\n",
      "→ ↓ ← ← ←\n",
      "↑ ↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ ↑ ←\n",
      "↑ ↑ ↑ ↑ ↑\n",
      "\n",
      "Optimal Value Function without Permutation:\n",
      "[[49.33333333 46.66666667 49.33333333 46.66666667 44.13333333]\n",
      " [46.66666667 49.33333333 46.66666667 44.13333333 44.42666667]\n",
      " [44.13333333 46.66666667 44.13333333 41.72666667  0.        ]\n",
      " [41.72666667 44.13333333 41.72666667 39.44033333 37.26831667]\n",
      " [ 0.         41.72666667 39.44033333 37.26831667 35.20490083]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.full((grid_size, grid_size), -0.2)\n",
    "initial_blue_pos = (0, 1)\n",
    "initial_green_pos = (0, 4)\n",
    "rewards[initial_blue_pos] = 5   # Initial Blue square\n",
    "rewards[initial_green_pos] = 2.5  # Initial Green square\n",
    "\n",
    "# Define terminal states\n",
    "terminal_states = [(4, 0), (2, 4)]\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "action_indices = {(-1, 0): 0, (1, 0): 1, (0, -1): 2, (0, 1): 3}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Function to get the next state and reward with permutation\n",
    "def get_next_state_reward(state, action, blue_pos, green_pos):\n",
    "    if state in terminal_states:\n",
    "        return (0, state, blue_pos, green_pos)\n",
    "    # Swap positions of blue and green squares with 0.1 probability\n",
    "    if random.uniform(0, 1) < 0.1:\n",
    "        blue_pos, green_pos = green_pos, blue_pos\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "        return (-0.5, state, blue_pos, green_pos)  # Stepping off the grid\n",
    "    if next_state == blue_pos:\n",
    "        return (5, next_state, blue_pos, green_pos)  # Reward for Blue square\n",
    "    if next_state == green_pos:\n",
    "        return (2.5, next_state, blue_pos, green_pos)  # Reward for Green square\n",
    "    return (-0.2, next_state, blue_pos, green_pos)  # Normal move\n",
    "\n",
    "# Function to get the next state and reward without permutation\n",
    "def get_next_state_reward_fixed(state, action, blue_pos, green_pos):\n",
    "    if state in terminal_states:\n",
    "        return (0, state, blue_pos, green_pos)\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "        return (-0.5, state, blue_pos, green_pos)  # Stepping off the grid\n",
    "    if next_state == blue_pos:\n",
    "        return (5, next_state, blue_pos, green_pos)  # Reward for Blue square\n",
    "    if next_state == green_pos:\n",
    "        return (2.5, next_state, blue_pos, green_pos)  # Reward for Green square\n",
    "    return (-0.2, next_state, blue_pos, green_pos)  # Normal move\n",
    "\n",
    "# Initialize policy and value function\n",
    "policy = np.ones((grid_size, grid_size, len(actions))) / len(actions)\n",
    "V = np.zeros((grid_size, grid_size))\n",
    "\n",
    "# Policy evaluation function\n",
    "def policy_evaluation(policy, V, blue_pos, green_pos, theta=1e-6, max_iterations=1000):\n",
    "    for _ in range(max_iterations):\n",
    "        delta = 0\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                v = V[i, j]\n",
    "                new_v = 0\n",
    "                for a in range(len(actions)):\n",
    "                    action = actions[a]\n",
    "                    prob = policy[i, j, a]\n",
    "                    reward, next_state, new_blue_pos, new_green_pos = get_next_state_reward((i, j), action, blue_pos, green_pos)\n",
    "                    new_v += prob * (reward + gamma * V[next_state[0], next_state[1]])\n",
    "                V[i, j] = new_v\n",
    "                delta = max(delta, abs(v - V[i, j]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Policy improvement function\n",
    "def policy_improvement(V, blue_pos, green_pos):\n",
    "    policy_stable = True\n",
    "    new_policy = np.zeros((grid_size, grid_size, len(actions)))\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            old_action = np.argmax(policy[i, j])\n",
    "            action_values = np.zeros(len(actions))\n",
    "            for a in range(len(actions)):\n",
    "                action = actions[a]\n",
    "                reward, next_state, new_blue_pos, new_green_pos = get_next_state_reward((i, j), action, blue_pos, green_pos)\n",
    "                action_values[a] = reward + gamma * V[next_state[0], next_state[1]]\n",
    "            new_action = np.argmax(action_values)\n",
    "            if old_action != new_action:\n",
    "                policy_stable = False\n",
    "            new_policy[i, j] = np.eye(len(actions))[new_action]\n",
    "    return new_policy, policy_stable\n",
    "\n",
    "# Policy iteration function\n",
    "def policy_iteration(policy, V, blue_pos, green_pos, max_iterations=1000):\n",
    "    iteration = 0\n",
    "    while iteration < max_iterations:\n",
    "        #print(f\"Iteration: {iteration}\")\n",
    "        V = policy_evaluation(policy, V, blue_pos, green_pos)\n",
    "        policy, policy_stable = policy_improvement(V, blue_pos, green_pos)\n",
    "        iteration += 1\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return policy, V\n",
    "\n",
    "# Policy evaluation function for fixed squares\n",
    "def policy_evaluation_fixed(policy, V, blue_pos, green_pos, theta=1e-6, max_iterations=1000):\n",
    "    for _ in range(max_iterations):\n",
    "        delta = 0\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                v = V[i, j]\n",
    "                new_v = 0\n",
    "                for a in range(len(actions)):\n",
    "                    action = actions[a]\n",
    "                    prob = policy[i, j, a]\n",
    "                    reward, next_state, new_blue_pos, new_green_pos = get_next_state_reward_fixed((i, j), action, blue_pos, green_pos)\n",
    "                    new_v += prob * (reward + gamma * V[next_state[0], next_state[1]])\n",
    "                V[i, j] = new_v\n",
    "                delta = max(delta, abs(v - V[i, j]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Policy improvement function for fixed squares\n",
    "def policy_improvement_fixed(V, blue_pos, green_pos):\n",
    "    policy_stable = True\n",
    "    new_policy = np.zeros((grid_size, grid_size, len(actions)))\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            old_action = np.argmax(policy[i, j])\n",
    "            action_values = np.zeros(len(actions))\n",
    "            for a in range(len(actions)):\n",
    "                action = actions[a]\n",
    "                reward, next_state, new_blue_pos, new_green_pos = get_next_state_reward_fixed((i, j), action, blue_pos, green_pos)\n",
    "                action_values[a] = reward + gamma * V[next_state[0], next_state[1]]\n",
    "            new_action = np.argmax(action_values)\n",
    "            if old_action != new_action:\n",
    "                policy_stable = False\n",
    "            new_policy[i, j] = np.eye(len(actions))[new_action]\n",
    "    return new_policy, policy_stable\n",
    "\n",
    "# Policy iteration function for fixed squares\n",
    "def policy_iteration_fixed(policy, V, blue_pos, green_pos, max_iterations=1000):\n",
    "    iteration = 0\n",
    "    while iteration < max_iterations:\n",
    "        #print(f\"Iteration: {iteration}\")\n",
    "        V = policy_evaluation_fixed(policy, V, blue_pos, green_pos)\n",
    "        policy, policy_stable = policy_improvement_fixed(V, blue_pos, green_pos)\n",
    "        iteration += 1\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return policy, V\n",
    "\n",
    "# Initial positions of blue and green squares\n",
    "blue_pos = (0, 1)\n",
    "green_pos = (0, 4)\n",
    "\n",
    "# Perform policy iteration for permutation scenario\n",
    "print(\"Policy Iteration with Permutation:\")\n",
    "optimal_policy_permutation, optimal_V_permutation = policy_iteration(policy.copy(), V.copy(), blue_pos, green_pos)\n",
    "\n",
    "# Perform policy iteration for fixed squares scenario\n",
    "print(\"\\nPolicy Iteration without Permutation:\")\n",
    "optimal_policy_fixed, optimal_V_fixed = policy_iteration_fixed(policy.copy(), V.copy(), blue_pos, green_pos)\n",
    "\n",
    "# Print the optimal policy for permutation scenario\n",
    "action_symbols = {(-1, 0): '↑', (1, 0): '↓', (0, -1): '←', (0, 1): '→'}\n",
    "print(\"\\nOptimal Policy with Permutation (using arrows):\")\n",
    "for i in range(grid_size):\n",
    "    row = []\n",
    "    for j in range(grid_size):\n",
    "        best_action_idx = np.argmax(optimal_policy_permutation[i, j])\n",
    "        best_action = actions[best_action_idx]\n",
    "        row.append(action_symbols[best_action])\n",
    "    print(\" \".join(row))\n",
    "\n",
    "# Print the optimal value function for permutation scenario\n",
    "print(\"\\nOptimal Value Function with Permutation:\")\n",
    "print(optimal_V_permutation)\n",
    "\n",
    "# Print the optimal policy for fixed squares scenario\n",
    "print(\"\\nOptimal Policy without Permutation (using arrows):\")\n",
    "for i in range(grid_size):\n",
    "    row = []\n",
    "    for j in range(grid_size):\n",
    "        best_action_idx = np.argmax(optimal_policy_fixed[i, j])\n",
    "        best_action = actions[best_action_idx]\n",
    "        row.append(action_symbols[best_action])\n",
    "    print(\" \".join(row))\n",
    "\n",
    "# Print the optimal value function for fixed squares scenario\n",
    "print(\"\\nOptimal Value Function without Permutation:\")\n",
    "print(optimal_V_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sasinda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
